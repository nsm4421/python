{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI0B5IWkNjLi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n64iSaUZNf5l"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DQn20WBNwLa"
      },
      "source": [
        "Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7i64CsqAyJ"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "  def __init__(self, emb_dim, drop_rate = 0.5):\r\n",
        "    super(Attention, self).__init__()\r\n",
        "\r\n",
        "    self.emb_dim = emb_dim\r\n",
        "    self.drop_rate = drop_rate\r\n",
        "    self.dropout = nn.Dropout(self.drop_rate)\r\n",
        "\r\n",
        "    self.W_query = nn.Linear(self.emb_dim, self.emb_dim)    # W_q\r\n",
        "    self.W_key = nn.Linear(self.emb_dim, self.emb_dim)    # W_k\r\n",
        "    self.W_value = nn.Linear(self.emb_dim, self.emb_dim)    # W_v\r\n",
        "    self.d_k = torch.FloatTensor([1]).double()    # scaling factor\r\n",
        "\r\n",
        "  def forward(self, query, key, value, mask = None, negative_inf = -1e10):\r\n",
        "    # Input Shape :  [batch size, sequence length, embedding dimension]\r\n",
        "\r\n",
        "    batch_size = query.shape[0]    \r\n",
        "    query = self.W_query(query).view(batch_size, -1, self.emb_dim)\r\n",
        "    key = self.W_key(key).view(batch_size, -1, self.emb_dim)\r\n",
        "    value = self.W_value(value).view(batch_size, -1, self.emb_dim)\r\n",
        "    # Shape : [batch size, sequence length, embedding dimension]\r\n",
        "\r\n",
        "    # Attention energy\r\n",
        "    attention_energy = torch.matmul(query, torch.transpose(key, 1, 2))\r\n",
        "    # [batch size, seq len, emb dim] x [batch size, emb dim, seq_len]\r\n",
        "    # ---> [batch size, seq len, seq len]\r\n",
        "    attention_energy /= torch.sqrt(self.d_k)\r\n",
        "    if mask is not None:\r\n",
        "      attention_energy = attention_energy.masked_fill(mask==0, -negative_inf)\r\n",
        "    attention_energy = torch.softmax(attention_energy, dim = -1)\r\n",
        "    \r\n",
        "    # Attention Score\r\n",
        "    attention_score = torch.matmul(self.dropout(attention_energy), value)\r\n",
        "    # [batch size, seq len, seq len] x [batch size, seq len, emb dim]\r\n",
        "    # ---> [batch size, seq len, emb dim]\r\n",
        "    \r\n",
        "    return attention_energy, attention_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0oNfGp1791P"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-voDpmnvDwQ",
        "outputId": "70fb8df0-dd43-4a76-f6f0-8f4557ee6882"
      },
      "source": [
        "a = torch.tensor([[[1,2,3], [4,5,6]], [[1,2,3], [4,5,6]], \r\n",
        "                  [[1,2,3], [4,5,6]], [[1,2,3], [4,5,6]], \r\n",
        "                  [[1,2,3], [4,5,6]], [[1,2,3], [4,5,6]]])\r\n",
        "print(f'batch size {a.shape[0]},  sequence length : {a.shape[1]},   embedding dim :{a.shape[2]}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size 6,  sequence length : 2,   embedding dim :3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iWnlo0UvFr_",
        "outputId": "7343497c-bf62-4ed2-d1ed-dc26364c8cac"
      },
      "source": [
        "tr_a = torch.transpose(a,1, 2)\r\n",
        "print(f'transpose of a has shape : {tr_a.shape}') "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transpose of a has shape : torch.Size([6, 3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZH9QudJvJFv",
        "outputId": "f82b9b66-6b54-4d2d-b368-b77cadb7c546"
      },
      "source": [
        "mat_mul = torch.matmul(a, tr_a)\r\n",
        "print(f'Matrix multiplication of a and its transpose has shape {mat_mul.shape}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix multiplication of a and its transpose has shape torch.Size([6, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1glHRTpHvP1D"
      },
      "source": [
        "at = Attention(emb_dim = 3).double()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giNhDpTzxxF-",
        "outputId": "ca83268c-af33-46f8-f061-8d757fa8fc40"
      },
      "source": [
        "a = a.double()\r\n",
        "print(f'attention enegery matrix shape : {at(a, a, a)[0].shape}')\r\n",
        "# [batch size, sequence length, sequence length]\r\n",
        "print(f'attention score shape : {at(a, a, a)[1].shape}')\r\n",
        "# [batch size, sequence length, embedding dim]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention enegery matrix shape : torch.Size([6, 2, 2])\n",
            "attention score shape : torch.Size([6, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETsIoW7_9HW4"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}